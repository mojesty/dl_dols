{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295825 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "y exkatvz jlajrehrvdethiaagbtejojwsxtnzpkhloriamej mrka  nptmdmazj e pqzkfkqq sn\n",
      "dcgeeeoxexidfdsvrj ueryvo xmogphhzi atixit tuh k y s c qs ten x  ytactdi ozy  s \n",
      "scuhyxdvcqllsn e cj dp ecrfiq at xvg xe lkeqt ce zpxj dipmiye fypbmhvsbctd tnkjp\n",
      "rpikpkaolvztugnoa olxlq e qxbtgoeteg daijclbxuythw ofiohx   hjjtslegdeawvupu   o\n",
      "jycq eiynt vireioylo arepcziil ryry yeyeeicsdljsnutmcfnynswhbdxc kuusrjamlsee nm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.33\n",
      "Average loss at step 100: 2.599497 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.28\n",
      "Validation set perplexity: 10.88\n",
      "Average loss at step 200: 2.259934 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.63\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 300: 2.102393 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 400: 1.995464 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.933988 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 600: 1.908572 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 700: 1.855036 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800: 1.815429 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 900: 1.826735 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.821721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "hen elecpute of the momovul axballest of chase conce roctes libord the comple ou\n",
      "pobbes invriated free cellial romua or chirt leterigy in uniar ergecal a mal one\n",
      "k voy he zara of the reewaolionod had brearia and comprepte will or the zero zer\n",
      "n per deration and the secpiolan audint the beather on one centels howm each of \n",
      "bia ul a kenejeame recoudis anole letweed fold of a pulf cammate on the syveate \n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.771979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.748925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.732431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.747940 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.736886 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.744742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.710430 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.673516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1900: 1.643563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2000: 1.697225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "enter to besing it his but danda penshogh notoor movains tennd rond be ligh its \n",
      "dinic inenatives the papdationy teattary in terms pantinned to ilessing a longun\n",
      "s and alkby the and holk valled in anno issork incolyve dispilled oparable quati\n",
      "gensive anturisn bg simm geople and the nativions ventienal rebouthevenning erve\n",
      "binad informations one ex parts wys in gipctudes thaakge emperagentlants collect\n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2100: 1.685157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2200: 1.682610 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2300: 1.640094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2400: 1.662652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2500: 1.681899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2600: 1.655508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.657967 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2800: 1.653019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.652470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.652567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "on rusixally duire dealses of the encordating that abe redray dat sowe by patted\n",
      "factary shout theulay bail cag desibor engidaning indioum new ismused clayings d\n",
      "actsent of theres onition to f rexort travation and indianal deally one five six\n",
      "zers writhers axanderventias as dirscal sivelymon in economicable it one nine si\n",
      "wital and a discomes in the fristratally and one nine seven six zero why eight s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3100: 1.629592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.649766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.638272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3400: 1.671501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.662495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.673503 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.645221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.641997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3900: 1.638318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.650548 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "am whas othorm s repensed the cancreate theoln been up its camprisal possifer ca\n",
      "gen by secus elepidance the christed proxucted to chyina witsiard dencurration s\n",
      "re over to arricated from of has opservers deveter consider in music bloyed were\n",
      "giensifenting to the combect place the solver six is arey p consides for speachi\n",
      "n be many law this s foncher succlation argeres locat of earls not long photedoc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.629971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.637130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4300: 1.612936 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.610547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4500: 1.617755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4600: 1.613978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4700: 1.622535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.630969 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.633822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.604496 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "le who with brangs sumber is bay successons accorded externed that life like rea\n",
      "fustlamed or one nine three spangeba one nine five nine eight zero his own capul\n",
      "chary theer to francipe he poot non oppaiblethor typagrour alligersing s mojag h\n",
      "le jural davia micrage appire with a previder the bockey the world hant rosjoh s\n",
      "ustic divery misoplemi andly bas and maden is the humertic and rebosa yearalianc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Gates\n",
    "    xg = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mg = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bg = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matr = tf.matmul(i, xg) + tf.matmul(o, mg) + bg\n",
    "        \n",
    "        input_gate = tf.sigmoid(matr[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matr[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update = matr[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(matr[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.matmul(tf.concat(outputs,0), w)\n",
    "        logits += b#tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = logits, labels = tf.concat(train_labels, 0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-f998ac34a783>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.299900 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.11\n",
      "================================================================================\n",
      "lpshwir rzh vrp apgchh kbddlhelleodco izgen eejndrph tcr b  rqwscoemz zebtgydi  \n",
      "del y ywnmtpz h unsi jk qrayaeoouoccq gcvgf wfydoiionzjapbftg  koggekul siundoce\n",
      "aiiieinafktesvpt s  qhsv txasgjbsncetf   ce  lms amd emzkc blr tqrtrqg mazj lrtj\n",
      "ktgdcdo rigiknuqitntheade hier ilevaphrr gzj juan iiet erhsonkjvteo thtueqpgja o\n",
      "tdonrf  c me pottex  b rewkikao mm jrwm tprbrsaagoeeoi  fqwxarqtwtnkvtgobdi k h \n",
      "================================================================================\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 100: 2.593492 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.70\n",
      "Validation set perplexity: 10.87\n",
      "Average loss at step 200: 2.279996 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.84\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 300: 2.105279 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.012531 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.956199 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 600: 1.916376 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.872776 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.851978 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 900: 1.822904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.779329 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "================================================================================\n",
      "veresil plusents for a seleppance oriervent incope se non yef is the preden and \n",
      "sed of deadp uniqearised as ewhems harded or is hts but in these a sens hasngz a\n",
      "y ol lemos of the eunfics adredo the all insuinition of vared use hoind a dupcha\n",
      " the secres carde versed earees of are a de apt the the crebers leverge in mberp\n",
      "wic to here dustame pross kease the libe relarate and soun ban none stanrar or p\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.776523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1200: 1.734709 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1300: 1.731092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.704584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.716378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1600: 1.747269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1700: 1.722103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.737783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1900: 1.700008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 2000: 1.693267 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "h an lisam his a five when de had hist year monay germent to at arestrally the b\n",
      "x buiter ply jehuring one seven tinetions nown in urevess and plai beet setts di\n",
      "alt the sersover one five one nine three six two hlished the bush in the coyt pr\n",
      "filing nequre be that store usencic and hadder as wage an ascess and for is in t\n",
      "ids and in the quicia the the dor of the kde the seven enn set chere history reh\n",
      "================================================================================\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2100: 1.683348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2200: 1.674478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2300: 1.666690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.682427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2500: 1.683282 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2600: 1.637744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2700: 1.634478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2800: 1.662282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2900: 1.664107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3000: 1.691231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "qued these the francic fifition found to healtzherd qurger one five zero zero ze\n",
      "t erapone formant free will though inciaple funce ruber on americans jalian moda\n",
      "ult  fignar in from separeng other juser nine s her guisable deen from datbom po\n",
      "ulter steined in lige sceal lacagua port all noung treeu prohressing ahrition si\n",
      "in moves the torshil redective cleculine brobf and morifises instete the word po\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3100: 1.659170 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3200: 1.639141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.634804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3400: 1.647184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3500: 1.640784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3600: 1.632310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.626298 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3800: 1.601828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3900: 1.611453 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4000: 1.604263 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "verbir in seven and superate to and in the dooc includersh combrun americane of \n",
      "led gue the fograted win that arppey ishanded of two runnows for moduress eight \n",
      "ackual henployd defined descrions of which df difference avix in the cant last l\n",
      "jccont cause and e and trailiemations autism polishn one seven zero ginged franc\n",
      "ings informi many he tominative iressive from coint trested his be celual b s th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4100: 1.612671 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4200: 1.637066 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.644664 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4400: 1.621904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4500: 1.633257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4600: 1.618029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.628814 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.632501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4900: 1.625127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5000: 1.638481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "r ray the groms oxigani varinegiations and the mixibay reconversed to one eight \n",
      "orphers weado to one nine nine six zero zero zero one sixuasib vissora grears be\n",
      "rie save subnindin the quovarglion the britage were outits of one poyed than gre\n",
      "well a srust arover to for beroets bative friend in triq it the chayparop see is\n",
      "fern usingur and is colen sirits shown book flatian respanity iess as the marine\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Gates\n",
    "    xg = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mg = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bg = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matr = tf.matmul(i, xg)\n",
    "        matr += tf.matmul(o, mg) + bg\n",
    "        \n",
    "        input_gate = tf.sigmoid(matr[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matr[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update = matr[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(matr[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))    \n",
    "    \n",
    "    #embeddings\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeddings = []\n",
    "    for item in train_inputs:\n",
    "        embedded = tf.nn.embedding_lookup(embeddings, item)\n",
    "        train_embeddings.append(embedded)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeddings:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.matmul(tf.concat(outputs,0), w)\n",
    "        logits += b#tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = logits, labels = tf.concat(train_labels, 0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embedded = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed1d(matr):\n",
    "    res =  matr.argmax(axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-a5d20acfb3c3>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.297080 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "ln apdle xlcqcefgnphsgdeixlfyhedce yo joricnx   onb fnv rdc nz jcodeoiink  jga  \n",
      "xke keifn ddcyao tiebu o ie yistyn nmawtp et zgtklb  nf hwwnckcglwdo redcafhman \n",
      "imbeoj ojouofionojr  ngk v n kx yehrcudedjckhoeqnxv  gilheoo uneenhryyed owrj se\n",
      "k s n idkzaacofvxcyionytpeyedi yroykaiheh tqtusir tbal te r eha ya c ldnsokhouyl\n",
      "sfevmwagnwmrizniseye ui vh mamo elogu chz uchqlrkmasxdhgoheporijenjz  md t oulpb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.497750 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.81\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 200: 2.159925 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.14\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 300: 2.039804 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 400: 1.964564 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 500: 1.956955 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 600: 1.907039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 700: 1.884217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 800: 1.860007 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.837650 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 1000: 1.821327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "wer from others mamby nation rameous ner nuece i his priness herrestry the the o\n",
      "ors its e fre cl the uniters one niveropes muside widthne in houte incrided as c\n",
      "zers milvegy and cam in from one emn il at as of has deber onobch two ediling wi\n",
      "ing for preater only or garch ceptem impo on fimbryp of the press fright f one s\n",
      "raf clans mio ousumed struch with of habria one semters seven zero seven to scom\n",
      "================================================================================\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1100: 1.789997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.771529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.771013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.754089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1500: 1.778585 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1600: 1.766833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1700: 1.748753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1800: 1.742524 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1900: 1.742274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2000: 1.731533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "zeery the world has harranater of bandare fromes be dregant a listicily leden th\n",
      "nition unders the first squaptyr three one seven seven bustorigity drocelg clea \n",
      "err citily of misicity flanew whiddes was piterity to de lam openly buces hevary\n",
      "hed knuphory of the woch their the four indos hercree hes dinz gnomicn the cesnt\n",
      "d was linl chembichtimily they the earth oggn the fev nivelatutionous of thembll\n",
      "================================================================================\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 2100: 1.730655 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 2200: 1.724581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 2300: 1.724086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 2400: 1.726605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2500: 1.716663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 2600: 1.688666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2700: 1.695078 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.712918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2900: 1.692596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3000: 1.698950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "x with eliver mefg apallbo in tonawion at on five be fighmer publism but as an l\n",
      "an with comenial afmance named unitare and makon made by selectely calen gorngel\n",
      "loses primary in increasm by catures an eurincland nine seven five eight eight f\n",
      "ha and designe on the andrys non six one nine five nine zero and to bessupinate \n",
      "land by of car two one eingeends in the agaim uincollers dombiphopy use to spana\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 3100: 1.693071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3200: 1.699535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 3300: 1.698791 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 3400: 1.683321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 3500: 1.656579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 3600: 1.705917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3700: 1.677234 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 3800: 1.681681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 3900: 1.672893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 4000: 1.695227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "jinations of jary textry for a bricawis sensions of as aldorged astheamy most of\n",
      " a jundir arou wech by the englison jerelk jornwlar torowed comyoguistions aboll\n",
      "geed been mosted war as eight alburial one nine two seven cheazel facio op whear\n",
      "k as releases about base and schinia accords to of alcoright swisa link fill top\n",
      "qutically active flown unite in pribrennsinus stalz up from the all lactions gla\n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 4100: 1.702028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 4200: 1.664061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 4300: 1.656318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 4400: 1.660171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 4500: 1.699064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 4600: 1.674092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 4700: 1.673495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 4800: 1.694033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 4900: 1.703001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 5000: 1.642460 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "ders a formes year brings oerinian deciss in the spiting patriek their for the b\n",
      "bert unionsyed to prodottment process one nine including christerity pluninctse \n",
      "god to bature he early figt reboved unifrides increan bea common the jut the tru\n",
      "anow riskives winscal the hemt prevismsian offered of mary to seleveen or usalhy\n",
      "promine is presencin of elevations not bay tods reportw ever eadly one nine the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = embed1d(batches[i])\n",
    "            feed_dict[train_labels[i]] = batches[i + 1]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: embed1d(feed)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: embed1d(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Gates\n",
    "    xg = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mg = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bg = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        print(i.shape)\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matr = tf.matmul(i, xg)\n",
    "        matr += tf.matmul(o, mg) + bg\n",
    "        \n",
    "        input_gate = tf.sigmoid(matr[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matr[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update = matr[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(matr[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "#     train_inputs = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in range(num_unrollings)]\n",
    "#     train_labels = [tf.placeholder(tf.int32, shape=[batch_size]) for _ in range(num_unrollings - 1)]\n",
    "    \n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeddings = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        e1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        #print(embedding_1.shape)\n",
    "        e2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embedding = tf.concat([e1, e2], 1)\n",
    "        #print('E, ', embeddings.shape, 'L ', embedding_1[0].shape,' ', embedding_1[1].shape)\n",
    "        train_embeddings.append(embedding)\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeddings:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.matmul(tf.concat(outputs,0), w)\n",
    "        logits += b#tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits = logits, labels = tf.concat(train_labels, 0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(3.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    embedding1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    embedding2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embedded = tf.concat([embedding1, embedding2], 1)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embedded, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-76-08f7761cdb62>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.294987 learning rate: 3.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "pfhwcpadfouiczcfqeva oqcjkkuzddxspvvfhxlskms pelvqswuxbfls wxgpix bld xnho aflvov\n",
      "fpnymwbrurmmhtdporlfwpsiwsvgjbzehoyt ey u nbvxsy yvygqmbvogixchtqgwtcxoqyd rajv l\n",
      "ctobtdglggjlhgvic llertih xav ijtmyqksieitnvukijs zdxqzqbiokhvvycnzmqyiihwignkljo\n",
      "xfufnihzlvytungzabscnziubits enprbyn nejxoavdxhasdikyzmxzesmgvmw kfizcrsdirzuqmks\n",
      "fpthhfrruqzadvkt qlpl pczybikdfumgmigyg npgqcrqkdkmrq hld eochepbawofcoendwmdwebz\n",
      "================================================================================\n",
      "Validation set perplexity: 23.72\n",
      "Average loss at step 100: 2.613818 learning rate: 3.000000\n",
      "Minibatch perplexity: 10.48\n",
      "Validation set perplexity: 11.32\n",
      "Average loss at step 200: 2.290181 learning rate: 3.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 10.20\n",
      "Average loss at step 300: 2.162044 learning rate: 3.000000\n",
      "Minibatch perplexity: 8.94\n",
      "Validation set perplexity: 9.64\n",
      "Average loss at step 400: 2.078508 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 9.54\n",
      "Average loss at step 500: 2.027772 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 600: 2.010921 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 700: 1.928192 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 9.10\n",
      "Average loss at step 800: 1.921280 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 900: 1.913933 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 1000: 1.880536 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.03\n",
      "================================================================================\n",
      "rcohuhe  uhhhhuh hohhehkhlhohohehhhhfeehth auhhluhho li hluahihihhhihu  hiihhhsou\n",
      "tplleseilsher eeekeiteelspoe eooeeaeh eehsle e ehheeeleeneoeeenehh rehehearih lee\n",
      "upa ele   e r eh  hseelnee l a o e        u e     s   l le la l  p l  e    i l  a\n",
      "ld  ye aieeou dyioe e y ayoa   y  e y e yi  yeuue euo  eyiuoeo ye oo     u ya   y\n",
      "cterohiihrsheo uei hirio io hirlrp hihr aisiol ihho   saihsueotosh oi  rerohi i  \n",
      "================================================================================\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 1100: 1.876004 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 1200: 1.874747 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 1300: 1.877667 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 1400: 1.876331 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 1500: 1.883631 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 1600: 1.847463 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 1700: 1.836468 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 1800: 1.810704 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 1900: 1.798895 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 2000: 1.800525 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "sxaooeaeeeeosoiaieeioeeaieeaieeaetoetiereeeeeeeeeeeereieeereeeeaeeeeoeeeexeereeoe\n",
      "cx               e        e     en  a          a    n                            \n",
      "nw ejieieieyeeeeeeieeeiieaeeeeeeaiieeeieeeaeyeeeeeieiiieeeeeeieeueeeeeyeeaeeaeeee\n",
      "aki   e    e     e        eyi   e  ib i   ee  ee       i y l    i ie        eiee \n",
      "hweeao i n   w n ioinoa  ahaan  annh e aannaela r  naaan r onan    y aana  oaoary\n",
      "================================================================================\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 2100: 1.788617 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 2200: 1.812045 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 2300: 1.809161 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 2400: 1.798612 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 2500: 1.767547 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 2600: 1.787753 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 2700: 1.788893 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 2800: 1.785256 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 2900: 1.781228 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 3000: 1.761263 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.17\n",
      "================================================================================\n",
      "gp    lie s ls  e   o  ellee   i  hl  ee elt  e seeece tl eelte  ellll l  e il ll\n",
      "dw aaaaaaazaaaaaaaaaaraaaa aaaaaaaaaaaaeaiaaaaraeaaaaaaaaeaiaaaaaaoeaaaaaaaoaaaae\n",
      "p wctdtohjecotwwffafdtccotnatwpbiwtbsfwvmecwwoftistftattsmbhoffjkmshipcxscwbwtwoa\n",
      "yzseeaoeaeeieuoihooeeeoeeoheeeioeiooooeeoeeeeeieaeeeeieeeiioeoeiieiieoeoiioiooeee\n",
      "yw  neninneinnhiienoanoninoneineoeaonyeanaeoaneeaeninaennyoeeneaieeaeeaeenaoneann\n",
      "================================================================================\n",
      "Validation set perplexity: 8.91\n",
      "Average loss at step 3100: 1.773778 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 3200: 1.770456 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 3300: 1.779644 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 3400: 1.761512 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.74\n",
      "Average loss at step 3500: 1.754527 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 3600: 1.749090 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 3700: 1.760770 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 3800: 1.779631 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 3900: 1.760695 learning rate: 3.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 4000: 1.768039 learning rate: 3.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "vfeee i cee  aoiee iio s ef  o  ee eowi weeoilt   loeeeleet  e oeil eei  oe  e  e\n",
      "jan lnnn   nunnrv nnn  nnknbnnnn rnrnnrnt nwl nnn nnnnnlnn  nznn rk nrlnnln rrs n\n",
      "iveeaeeeeeeeeeeeeeeeeeeeeeeeeeaeeeeeeeeeeeeeieeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeoeeee\n",
      "gieenoylnrlusnslua arloodanplotg aoaumkaonolnassolr morrsaauatanlpuoaausaracioaos\n",
      "eze aeeeoeoiieeeeeueeehieeoeeueeeeeeeeooeeeueiu oeeeieueeieooeeee eeeeo eee eehie\n",
      "================================================================================\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 4100: 1.763688 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 4200: 1.776770 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 4300: 1.755395 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 4400: 1.761264 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 4500: 1.775123 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 4600: 1.758101 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 4700: 1.746031 learning rate: 3.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 4800: 1.736875 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 4900: 1.746122 learning rate: 3.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 5000: 1.774539 learning rate: 0.300000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "bndodood oooot ooooooodaddooodo gr doo xoho oodtaoodoodwoo ooooogdoujo ooaocaoooo\n",
      "jasttrrrntnrlnlnlrnrnrrrntn nnlrrnnnnslltrnnnlnntnllllrlrllnlnllnnmllrrrplnrmnnnl\n",
      "bp   e tl  h  aepme      srem    o y p  sdel k prp d   sah r    dn       h ierg i\n",
      "qrayyyey eeyyeeieieieeeieeuiyeiueeie  ieee eeeiyieiiieei eeeeeeeeeiyeeeeeeeieiee \n",
      "u mcmwmonlwtiacarcsesodmcfhttsfvyapsbahoifkoaaaduktavcimteamrsnsotsiicsmbsaswooci\n",
      "================================================================================\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 5100: 1.733214 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 5200: 1.742490 learning rate: 0.300000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 5300: 1.734537 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 5400: 1.734699 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 5500: 1.714585 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 5600: 1.734497 learning rate: 0.300000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 5700: 1.708517 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 5800: 1.708397 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 5900: 1.701375 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 6000: 1.717761 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "lh                     y   e     e         e         e      e o           a eo   \n",
      "pgaeaiaeareaioeiy aeeiioaeeie ueeea i ieoeaae eeaeeiaeeaieeneoeeiiyiaaieaoei oyea\n",
      "fas  ss    n sm s s    nr ss   s  v  r  sns r    cslr  vn snvns nrnrrnmrv rms dbl\n",
      "jlye e aeiiyyeaao aaayiiaay iieayafiayaeieeeeeaiaaaaaaaeaiaaaiiiyieieaioiiyeaia i\n",
      "jryu o  y oe         oeou   o   s    a ou ooo i    ioo o aa uooe   uo   o ao   a \n",
      "================================================================================\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 6100: 1.734214 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 6200: 1.715838 learning rate: 0.300000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 6300: 1.684930 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 6400: 1.711210 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 6500: 1.704803 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 6600: 1.697285 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 6700: 1.717619 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 6800: 1.710653 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 6900: 1.740496 learning rate: 0.300000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 7000: 1.738268 learning rate: 0.300000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "an  dc  s  d td t s ds ddodddsddg d tstd g   sa g tittssg   sd ns  d dod td tissd\n",
      "nquiluusl uuuulimcuuuuuu muluuuiuuuuluuntuuuuuuuuuuuuubuu umnuuumlruxmuluusuuuuuu\n",
      "tjaor rnnxrorranaroarnlenxponoar lrxrrnrworroo brlpxxnnar oqoroxrnrrrinlo xerogwn\n",
      "nlyddydddddddddd ddddid  dd ddddddddyd yedddeilydiididd dydjdyfd iddddd ddddd idd\n",
      "bpib    i i         h  ii       ei  ca  r ea i i i ee    reei  i d  y h hshi i   \n",
      "================================================================================\n",
      "Validation set perplexity: 8.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = embed1d(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = embed1d(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([embed1d(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([embed1d(b[0]), embed1d(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
